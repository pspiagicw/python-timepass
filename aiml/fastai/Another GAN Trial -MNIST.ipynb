{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c52baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f503cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24363a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d7887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b083965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 28\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d221ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4d06f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 15:13:29.029322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.062114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.062303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.062837: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-08 15:13:29.084671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.084836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.084951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.789410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.789731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.789865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 15:13:29.789963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2658 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-02-08 15:13:29.790330: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "x = layers.Dense(128 * 14 * 14)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((14, 14, 128))(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf59e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 25088)             727552    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 25088)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 256)       819456    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 28, 28, 256)      1048832   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 3)         37635     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,910,787\n",
      "Trainable params: 5,910,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b505ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce82466",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "discriminator = tensorflow.keras.models.Model(discriminator_input, x)\n",
    "discriminator_optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.0008,clipvalue=1.0,decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer,loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf3be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 128)       3584      \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       262272    \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 5, 5, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1, 1, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 790,529\n",
      "Trainable params: 790,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3713dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "discriminator.trainable = False\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "gan_optimizer = tensorflow.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b67cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 28)]              0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 28, 28, 3)         5910787   \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 790529    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,701,316\n",
      "Trainable params: 5,910,787\n",
      "Non-trainable params: 790,529\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd4445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662b0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b279358",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x , train_y) , (test_x , test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a1bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8300358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape((-1,height,width,channels)).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d5b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e82d22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f0161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 15:13:33.078451: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss 0.44497770071029663\n",
      "adverserial loss 0.8578011393547058\n",
      "discriminator loss 0.006278226617723703\n",
      "adverserial loss 6.564140319824219\n",
      "discriminator loss 0.006830142345279455\n",
      "adverserial loss 5.810070991516113\n",
      "discriminator loss 0.006628791801631451\n",
      "adverserial loss 8.101269721984863\n",
      "discriminator loss 0.5787388682365417\n",
      "adverserial loss 1.2627407312393188\n",
      "discriminator loss 0.001227355794981122\n",
      "adverserial loss 18.632516860961914\n",
      "discriminator loss 0.007155274040997028\n",
      "adverserial loss 9.259364128112793\n",
      "discriminator loss 0.010220306925475597\n",
      "adverserial loss 7.236536979675293\n",
      "discriminator loss -0.0026436387561261654\n",
      "adverserial loss 11.399665832519531\n",
      "discriminator loss 0.5663478970527649\n",
      "adverserial loss 1.0187572240829468\n",
      "discriminator loss 0.0018093548715114594\n",
      "adverserial loss 10.184630393981934\n",
      "discriminator loss 0.033101048320531845\n",
      "adverserial loss 9.688679695129395\n",
      "discriminator loss 0.002028032438829541\n",
      "adverserial loss 10.56730842590332\n",
      "discriminator loss 0.00019134851754643023\n",
      "adverserial loss 13.943432807922363\n",
      "discriminator loss 0.003462852444499731\n",
      "adverserial loss 15.510233879089355\n",
      "discriminator loss 0.0011665649944916368\n",
      "adverserial loss 10.90513801574707\n",
      "discriminator loss 0.0007105000549927354\n",
      "adverserial loss 9.270007133483887\n",
      "discriminator loss 0.004023535642772913\n",
      "adverserial loss 8.792182922363281\n",
      "discriminator loss -0.0215675700455904\n",
      "adverserial loss 14.68824577331543\n",
      "discriminator loss 0.007827346213161945\n",
      "adverserial loss 14.761067390441895\n",
      "discriminator loss 0.0006075652199797332\n",
      "adverserial loss 13.903803825378418\n",
      "discriminator loss 0.005648217163980007\n",
      "adverserial loss 13.864446640014648\n",
      "discriminator loss 0.005364888813346624\n",
      "adverserial loss 8.529195785522461\n",
      "discriminator loss 0.00409685680642724\n",
      "adverserial loss 11.256219863891602\n",
      "discriminator loss 0.000504466996062547\n",
      "adverserial loss 12.725549697875977\n",
      "discriminator loss 0.5741024017333984\n",
      "adverserial loss 19.247623443603516\n",
      "discriminator loss 0.007987995631992817\n",
      "adverserial loss 9.281176567077637\n",
      "discriminator loss 0.0015305810375139117\n",
      "adverserial loss 10.972001075744629\n",
      "discriminator loss 0.0052083199843764305\n",
      "adverserial loss 8.8002290725708\n",
      "discriminator loss -0.001817622920498252\n",
      "adverserial loss 39.94718551635742\n",
      "discriminator loss -0.03084084764122963\n",
      "adverserial loss 20.5430908203125\n",
      "discriminator loss -0.0006207465776242316\n",
      "adverserial loss 33.522682189941406\n",
      "discriminator loss 4.192977030470502e-06\n",
      "adverserial loss 31.733917236328125\n",
      "discriminator loss -0.00020495010539889336\n",
      "adverserial loss 29.96676254272461\n",
      "discriminator loss -0.0012175142765045166\n",
      "adverserial loss 20.802717208862305\n",
      "discriminator loss -0.021319229155778885\n",
      "adverserial loss 32.077335357666016\n",
      "discriminator loss 0.004503593780100346\n",
      "adverserial loss 16.675146102905273\n",
      "discriminator loss 0.000732923683244735\n",
      "adverserial loss 23.463953018188477\n",
      "discriminator loss -0.0014802307123318315\n",
      "adverserial loss 24.13622283935547\n",
      "discriminator loss 13.203654289245605\n",
      "adverserial loss 2.2591943740844727\n",
      "discriminator loss -0.0002223578339908272\n",
      "adverserial loss 19.243736267089844\n",
      "discriminator loss -0.016315478831529617\n",
      "adverserial loss 19.471691131591797\n",
      "discriminator loss 0.0006110112299211323\n",
      "adverserial loss 28.00104331970215\n",
      "discriminator loss 0.0006058464059606194\n",
      "adverserial loss 14.219184875488281\n",
      "discriminator loss -0.1063046008348465\n",
      "adverserial loss 30.41188621520996\n",
      "discriminator loss -0.2831837236881256\n",
      "adverserial loss 52.2960205078125\n",
      "discriminator loss 7.313943933695555e-05\n",
      "adverserial loss 23.063369750976562\n",
      "discriminator loss -4.460314448806457e-05\n",
      "adverserial loss 33.294517517089844\n",
      "discriminator loss -2.7358009815216064\n",
      "adverserial loss 95.48194122314453\n",
      "discriminator loss -2.6988516765413806e-05\n",
      "adverserial loss 41.33984375\n",
      "discriminator loss 0.0005988534539937973\n",
      "adverserial loss 16.002965927124023\n",
      "discriminator loss -0.1759338080883026\n",
      "adverserial loss 47.88446044921875\n",
      "discriminator loss 4.372583134681918e-05\n",
      "adverserial loss 47.30482864379883\n",
      "discriminator loss 0.0010446388041600585\n",
      "adverserial loss 25.6263484954834\n",
      "discriminator loss -0.09982609748840332\n",
      "adverserial loss 44.219764709472656\n",
      "discriminator loss 0.00013960301293991506\n",
      "adverserial loss 52.86769485473633\n",
      "discriminator loss 4.2959544543919037e-07\n",
      "adverserial loss 38.78235626220703\n",
      "discriminator loss -0.0010001916671171784\n",
      "adverserial loss 82.23701477050781\n",
      "discriminator loss 0.0003446762857493013\n",
      "adverserial loss 26.100177764892578\n",
      "discriminator loss 8.900931788957678e-06\n",
      "adverserial loss 59.633216857910156\n",
      "discriminator loss 2.1462828954099678e-05\n",
      "adverserial loss 40.741451263427734\n",
      "discriminator loss -0.00014207142521627247\n",
      "adverserial loss 51.65507125854492\n",
      "discriminator loss 0.2952914535999298\n",
      "adverserial loss 158.7261505126953\n",
      "discriminator loss 1.1517226994328666e-07\n",
      "adverserial loss 61.276031494140625\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    stop = start + batch_size\n",
    "    real_images = train_x[start:stop]\n",
    "    \n",
    "    combined_images = np.concatenate([generated_images,real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size,1)),\n",
    "                            np.zeros((batch_size,1))])\n",
    "    labels += 0.05 * discriminator.train_on_batch(combined_images , labels)\n",
    "    d_loss = discriminator.train_on_batch(combined_images , labels)\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size,1))\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors,misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(train_x) - batch_size:\n",
    "        start = 0\n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan-mnist.h5')\n",
    "        \n",
    "        print('discriminator loss', d_loss)\n",
    "        print('adverserial loss' , a_loss)\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0] * 255 , scale=False)\n",
    "        img.save(os.path.join(save_dir,'generated_digit' + str(step) + '.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0] * 255 , scale=False)\n",
    "        img.save(os.path.join(save_dir , 'real_digit' + str(step) + '.png'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92c087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
