{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c52baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f503cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24363a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d7887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b083965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d221ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4d06f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 14:04:22.334980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:22.399202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:22.399383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:22.399897: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-08 14:04:22.401479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:22.401619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:22.401731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:23.135101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:23.135264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:23.135374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-08 14:04:23.135470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2658 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-02-08 14:04:23.135661: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf59e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32768)             1081344   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 32768)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 256)       819456    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      1048832   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 3)         37635     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b505ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce82466",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "discriminator = tensorflow.keras.models.Model(discriminator_input, x)\n",
    "discriminator_optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.0008,clipvalue=1.0,decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer,loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf3be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 128)       3584      \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 30, 30, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 128)       262272    \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 2, 2, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3713dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "discriminator.trainable = False\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "gan_optimizer = tensorflow.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b67cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 32, 32, 3)         6264579   \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 790913    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,055,492\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 790,913\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd4445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662b0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b279358",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x , train_y) , (test_x , test_y) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8b466fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[train_y.flatten() == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10a1bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8300358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape((-1,height,width,channels)).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d5b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82d22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56f0161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'frogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44de0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss 0.3317388594150543\n",
      "adverserial loss 1.6023927927017212\n",
      "discriminator loss 0.39606648683547974\n",
      "adverserial loss 0.9372115135192871\n",
      "discriminator loss 0.5347632765769958\n",
      "adverserial loss 1.2676079273223877\n",
      "discriminator loss 0.4621354937553406\n",
      "adverserial loss 1.744518518447876\n",
      "discriminator loss 0.45870381593704224\n",
      "adverserial loss 1.0795338153839111\n",
      "discriminator loss 0.5174083113670349\n",
      "adverserial loss 0.9963424801826477\n",
      "discriminator loss 0.49775823950767517\n",
      "adverserial loss 0.7556710839271545\n",
      "discriminator loss 0.6083162426948547\n",
      "adverserial loss 0.8025890588760376\n",
      "discriminator loss 0.48227959871292114\n",
      "adverserial loss 0.5353943705558777\n",
      "discriminator loss 0.34673720598220825\n",
      "adverserial loss 0.9571229815483093\n",
      "discriminator loss 0.46261292695999146\n",
      "adverserial loss 1.4432169198989868\n",
      "discriminator loss 0.4333380162715912\n",
      "adverserial loss 1.1500861644744873\n",
      "discriminator loss 0.3510391414165497\n",
      "adverserial loss 1.3802120685577393\n",
      "discriminator loss 0.5486578941345215\n",
      "adverserial loss 0.7582998871803284\n",
      "discriminator loss 0.38578876852989197\n",
      "adverserial loss 1.7413218021392822\n",
      "discriminator loss 0.5039463043212891\n",
      "adverserial loss 1.1284157037734985\n",
      "discriminator loss 0.6081336736679077\n",
      "adverserial loss 0.5024315118789673\n",
      "discriminator loss 0.31998345255851746\n",
      "adverserial loss 0.5742776989936829\n",
      "discriminator loss 0.44037190079689026\n",
      "adverserial loss 0.9700592160224915\n",
      "discriminator loss 0.18077561259269714\n",
      "adverserial loss 3.614962100982666\n",
      "discriminator loss 0.32836678624153137\n",
      "adverserial loss 2.30092716217041\n",
      "discriminator loss 0.24475042521953583\n",
      "adverserial loss 1.7477947473526\n",
      "discriminator loss 0.38758108019828796\n",
      "adverserial loss 1.460505723953247\n",
      "discriminator loss 0.4415942132472992\n",
      "adverserial loss 1.1410144567489624\n",
      "discriminator loss 0.4618217349052429\n",
      "adverserial loss 0.944455623626709\n",
      "discriminator loss 0.3821203112602234\n",
      "adverserial loss 1.8236976861953735\n",
      "discriminator loss 0.38101014494895935\n",
      "adverserial loss 1.153342604637146\n",
      "discriminator loss 0.4586006999015808\n",
      "adverserial loss 1.1196762323379517\n",
      "discriminator loss 0.41304725408554077\n",
      "adverserial loss 1.5204566717147827\n",
      "discriminator loss 0.5472348928451538\n",
      "adverserial loss 0.9559168815612793\n",
      "discriminator loss 0.4569524824619293\n",
      "adverserial loss 1.3996102809906006\n",
      "discriminator loss 0.18777057528495789\n",
      "adverserial loss 2.210977792739868\n",
      "discriminator loss 0.4656258523464203\n",
      "adverserial loss 0.6698134541511536\n",
      "discriminator loss 0.6533623933792114\n",
      "adverserial loss 1.0443427562713623\n",
      "discriminator loss 0.18198367953300476\n",
      "adverserial loss 2.2594993114471436\n",
      "discriminator loss 0.5866442918777466\n",
      "adverserial loss 0.7734248042106628\n",
      "discriminator loss 0.29214417934417725\n",
      "adverserial loss 1.608194351196289\n",
      "discriminator loss 0.4395202100276947\n",
      "adverserial loss 1.5157732963562012\n",
      "discriminator loss 0.23007602989673615\n",
      "adverserial loss 1.1065698862075806\n",
      "discriminator loss 0.410336971282959\n",
      "adverserial loss 1.090909481048584\n",
      "discriminator loss 0.39434200525283813\n",
      "adverserial loss 1.493011474609375\n",
      "discriminator loss 0.3058457374572754\n",
      "adverserial loss 1.029138207435608\n",
      "discriminator loss 0.6204113960266113\n",
      "adverserial loss 1.0074464082717896\n",
      "discriminator loss 0.38113754987716675\n",
      "adverserial loss 1.0061959028244019\n",
      "discriminator loss 0.3343741297721863\n",
      "adverserial loss 1.0092477798461914\n",
      "discriminator loss 0.28627103567123413\n",
      "adverserial loss 1.874962568283081\n",
      "discriminator loss 0.2520780563354492\n",
      "adverserial loss 1.7286990880966187\n",
      "discriminator loss 0.10290898382663727\n",
      "adverserial loss 3.104903221130371\n",
      "discriminator loss 0.3385831415653229\n",
      "adverserial loss 1.3726472854614258\n",
      "discriminator loss 0.33808404207229614\n",
      "adverserial loss 1.5900352001190186\n",
      "discriminator loss 0.2812691032886505\n",
      "adverserial loss 1.424600601196289\n",
      "discriminator loss 0.17737829685211182\n",
      "adverserial loss 1.9368469715118408\n",
      "discriminator loss 0.23762044310569763\n",
      "adverserial loss 2.5549352169036865\n",
      "discriminator loss 0.2581078112125397\n",
      "adverserial loss 2.5528194904327393\n",
      "discriminator loss 0.26699280738830566\n",
      "adverserial loss 1.2384741306304932\n",
      "discriminator loss 0.3859112560749054\n",
      "adverserial loss 1.482627272605896\n",
      "discriminator loss 0.2483331710100174\n",
      "adverserial loss 4.448053359985352\n",
      "discriminator loss 0.1135016456246376\n",
      "adverserial loss 3.2815425395965576\n",
      "discriminator loss 0.4608074128627777\n",
      "adverserial loss 0.8058241009712219\n",
      "discriminator loss 0.20990929007530212\n",
      "adverserial loss 3.6725316047668457\n",
      "discriminator loss 0.015782365575432777\n",
      "adverserial loss 8.359286308288574\n",
      "discriminator loss 0.16275718808174133\n",
      "adverserial loss 3.045644521713257\n",
      "discriminator loss 0.09941469877958298\n",
      "adverserial loss 3.9600601196289062\n",
      "discriminator loss 0.18871304392814636\n",
      "adverserial loss 4.5145087242126465\n",
      "discriminator loss 0.28980180621147156\n",
      "adverserial loss 1.0894181728363037\n",
      "discriminator loss 0.06521718949079514\n",
      "adverserial loss 4.632910251617432\n",
      "discriminator loss 0.32866963744163513\n",
      "adverserial loss 2.359482526779175\n",
      "discriminator loss 0.25141042470932007\n",
      "adverserial loss 1.8226172924041748\n",
      "discriminator loss 0.8030887842178345\n",
      "adverserial loss 1.7540528774261475\n",
      "discriminator loss 0.022466357797384262\n",
      "adverserial loss 5.2957868576049805\n",
      "discriminator loss 0.37895703315734863\n",
      "adverserial loss 2.982187271118164\n",
      "discriminator loss 0.09727823734283447\n",
      "adverserial loss 4.352758884429932\n",
      "discriminator loss 0.0191025473177433\n",
      "adverserial loss 4.734292984008789\n",
      "discriminator loss 0.19938969612121582\n",
      "adverserial loss 2.409231424331665\n",
      "discriminator loss 0.2023482322692871\n",
      "adverserial loss 1.7854244709014893\n",
      "discriminator loss 0.2121489942073822\n",
      "adverserial loss 3.684234619140625\n",
      "discriminator loss 0.08075471222400665\n",
      "adverserial loss 4.529587268829346\n",
      "discriminator loss 0.3352828025817871\n",
      "adverserial loss 1.5025898218154907\n",
      "discriminator loss 0.27940115332603455\n",
      "adverserial loss 1.2261823415756226\n",
      "discriminator loss 0.030364617705345154\n",
      "adverserial loss 4.674855709075928\n",
      "discriminator loss 0.24661560356616974\n",
      "adverserial loss 1.7911796569824219\n",
      "discriminator loss 0.09121973812580109\n",
      "adverserial loss 2.9429280757904053\n",
      "discriminator loss 0.4014614224433899\n",
      "adverserial loss 1.2995887994766235\n",
      "discriminator loss 0.43734264373779297\n",
      "adverserial loss 6.4848151206970215\n",
      "discriminator loss 0.23606689274311066\n",
      "adverserial loss 2.8852689266204834\n",
      "discriminator loss 0.2634176015853882\n",
      "adverserial loss 1.8255020380020142\n",
      "discriminator loss 0.31577372550964355\n",
      "adverserial loss 3.0070412158966064\n",
      "discriminator loss 0.04167122393846512\n",
      "adverserial loss 6.272420406341553\n",
      "discriminator loss 0.09207366406917572\n",
      "adverserial loss 3.729679822921753\n",
      "discriminator loss 0.11239907890558243\n",
      "adverserial loss 3.5628247261047363\n",
      "discriminator loss 0.061985813081264496\n",
      "adverserial loss 4.408272743225098\n",
      "discriminator loss 0.34171685576438904\n",
      "adverserial loss 2.040752410888672\n",
      "discriminator loss 0.23011517524719238\n",
      "adverserial loss 2.0455615520477295\n",
      "discriminator loss 0.1069377064704895\n",
      "adverserial loss 5.024811744689941\n",
      "discriminator loss 0.03526350110769272\n",
      "adverserial loss 9.19487190246582\n",
      "discriminator loss 0.22753527760505676\n",
      "adverserial loss 3.2574119567871094\n",
      "discriminator loss 0.180288627743721\n",
      "adverserial loss 2.7832958698272705\n",
      "discriminator loss 0.14001940190792084\n",
      "adverserial loss 2.7888851165771484\n",
      "discriminator loss 0.3125137686729431\n",
      "adverserial loss 1.3797681331634521\n",
      "discriminator loss 0.310523122549057\n",
      "adverserial loss 2.8995563983917236\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    stop = start + batch_size\n",
    "    real_images = train_x[start:stop]\n",
    "    \n",
    "    combined_images = np.concatenate([generated_images,real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size,1)),\n",
    "                            np.zeros((batch_size,1))])\n",
    "    labels += 0.05 * discriminator.train_on_batch(combined_images , labels)\n",
    "    d_loss = discriminator.train_on_batch(combined_images , labels)\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size,1))\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors,misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(train_x) - batch_size:\n",
    "        start = 0\n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        \n",
    "        print('discriminator loss', d_loss)\n",
    "        print('adverserial loss' , a_loss)\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0] * 255 , scale=False)\n",
    "        img.save(os.path.join(save_dir,'generated_frog' + str(step) + '.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0] * 255 , scale=False)2\n",
    "        img.save(os.path.join(save_dir , 'real_frog' + str(step) + '.png'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e38b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
